{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c7da05-0438-41ce-a3e9-69bee9103a27",
   "metadata": {},
   "source": [
    "# Lecture 10: Procesamiento de Lenguaje Natural - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822f61f-7172-4039-9957-5966f7466378",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34afbd-bd77-42f6-b9a7-361134122710",
   "metadata": {},
   "source": [
    "Los computadores no son seres con inteligencia y comprensión avanzada, a menos que se les programe explícitamente para eso. Por lo general, los computadores tienden a ser bastante malos para difereciar caracteres en comparación con los humanos. Por ejemplo, para un computador la letra \"a\" es diferente a la letra \"A\". Así mismo, la frase \"Bad Bunny lanzará un álbum hoy\" es diferente a \"bad bunny lanzará un álbum hoy\". \n",
    "\n",
    "Probemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1563e669-1780-443d-9792-6b81dde30edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparando una letra minúscula con la mayúscula\n",
    "\"a\" == \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8097a9fd-4f83-4c13-af9d-0ae409f49430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparando una misma frase, pero con diferencias en mayúsculas\n",
    "\"Bad Bunny lanzará un álbum hoy\" == \"bad bunny lanzará un álbum hoy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b5594-7cbe-41ed-8fec-e87439aafa62",
   "metadata": {},
   "source": [
    "Los humanos son bastante buenos reconociendo que las dos frases de arriba son iguales, pero los computadores no. Por este hecho, normalmente en las bases de datos podemos encontrar errores \n",
    "\n",
    "en muchas ocasiones debemos ayudarle a un computador a entender nuestro lenguaje: el _lenguaje natural_. Esto se llama **Procesamiento de Lenguaje Natural**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb340d-621b-41b6-bc61-72e4ea47e4f2",
   "metadata": {},
   "source": [
    "Para explorar esta área de la Ciencia de Datos, vamos a jugar el rol de un consultor de negocios para pequeñas y medianas empresas con un considerable número de consumidores. Algunos ejemplos de empresas que utilizan este tipo de procesamiento son empresas de moda, distribuidores de equipos para hobbies, el procesamiento de documentos legales o testimonios de víctimas del conflicto, entre muchos otros.\n",
    "\n",
    "Algunos casos relevantes de NLP son Google, Bing, DuckDuckGo y otros motores de búsqueda para otorgar buenas respuestas a búsquedas vagas o mal escritas. También resaltan las traducciones automáticas y subtítulos generados automáticamente por YouTube.\n",
    "\n",
    "Los usos más comunes del NLP son:\n",
    "\n",
    "* Ayudar a los computadores a entender el habla humana (como Alexa, Siri, entre otros).\n",
    "* Traducir automáticamente entre diferentes tipos de lenguajes naturales (como Google Translate).\n",
    "* Aplicar automáticamente categorías para textos (como los detectores de spam o algunos algoritmos de Machine Learning e IA).\n",
    "* Lectura de textos en altavoz (Kindle, Google Translate).\n",
    "* Detección de emociones de textos y opiniones en redes sociales y otras páginas web.\n",
    "\n",
    "El objetivo en este caso es manipular una base de datos de reseñas en Internet y elegir características importantes para facilitar el análisis y modelamiento de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee15f7b-1c87-4971-8e39-2e9c0f5c093a",
   "metadata": {},
   "source": [
    "### 1.1. Retos del NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40352b-0acd-4e2b-b282-ad4cd8c9e612",
   "metadata": {},
   "source": [
    "Los retos del procesamiento de lenguaje natural más importantes son:\n",
    "\n",
    "* **Dimensionalidad extremadamente alta:** Don Quijote de la Mancha tiene 2'034.611 caracteres y 377.032 palabras. Si el computador considerara cada palabra como una variable (que es lo más común), sería imposible realizar un modelo con esa cantidad de variables. Ello tendría varios problemas:\n",
    "    - Requeriría mucho poder computacional.\n",
    "    - Acercamientos básicos de ML e IA tendrían un rendimiento terrible por la alta dimensionalidad.\n",
    "    - Estos acercamientos no captarían relaciones importantes entre las palabras y no podrían diferenciar entre \"don't\" y \"do not\" en inglés.\n",
    "    \n",
    "* **Los textos dependen del contexto:** muchas palabras tienen un uso diferente dependiendo de dónde se utilizan. Por ejemplo, la palabra \"suerte\" puede significar una causa o fuerza que determina el destino de alguien, el destino mismo o, en Colombia, una manera de despedirse de alguien.\n",
    "\n",
    "Para poder atender apropiadamente estos problemas, debemos utilizar librerías y toda _suerte_ de procesos para procesar bien este tipo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebc776-2faf-4d2a-98b7-45d4ca068933",
   "metadata": {},
   "source": [
    "## 2. Importación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519691ee-fdd6-4279-b948-4d2689c7f09f",
   "metadata": {},
   "source": [
    "En esta ocasión, vamos a utilizar una base de datos de opiniones de Yelp!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e33a41-647a-4d49-95dd-ebb73f656f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n6QzIUObkYshz4dz2QRJTw</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>VR6GpWIda3SfvPC-lg9H3w</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MV3CcKScW05u5LVfF6ok0g</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>CKC0-MOWMqoeWf6s-szl8g</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Lester's is located in a beautiful neighborhoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IXvOzsEMYtiJI0CARmj77Q</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>ACFtxLv8pGrrxMm6EgjreA</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Love coming here. Yes the place always needs t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L_9BTb55X0GDtThi6GlZ6w</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>s2I_Ni76bjJNK9yG60iD-Q</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Had their chocolate almond croissant and it wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  vkVSCC7xljjrAI4UGfnKEQ  bv2nCi5Qv5vroFiqKGopiw  AEx2SYEUJmTxVVB18LlCwA   \n",
       "1  n6QzIUObkYshz4dz2QRJTw  bv2nCi5Qv5vroFiqKGopiw  VR6GpWIda3SfvPC-lg9H3w   \n",
       "2  MV3CcKScW05u5LVfF6ok0g  bv2nCi5Qv5vroFiqKGopiw  CKC0-MOWMqoeWf6s-szl8g   \n",
       "3  IXvOzsEMYtiJI0CARmj77Q  bv2nCi5Qv5vroFiqKGopiw  ACFtxLv8pGrrxMm6EgjreA   \n",
       "4  L_9BTb55X0GDtThi6GlZ6w  bv2nCi5Qv5vroFiqKGopiw  s2I_Ni76bjJNK9yG60iD-Q   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      5  2016-05-28  Super simple place but amazing nonetheless. It...   \n",
       "1      5  2016-05-28  Small unassuming place that changes their menu...   \n",
       "2      5  2016-05-28  Lester's is located in a beautiful neighborhoo...   \n",
       "3      4  2016-05-28  Love coming here. Yes the place always needs t...   \n",
       "4      4  2016-05-28  Had their chocolate almond croissant and it wa...   \n",
       "\n",
       "   useful  funny  cool  \n",
       "0       0      0     0  \n",
       "1       0      0     0  \n",
       "2       0      0     0  \n",
       "3       0      0     0  \n",
       "4       0      0     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "# Cargando la base de datos\n",
    "data = pd.read_csv('sdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944d05c-3d85-4889-bd5a-0bc40b3d1d12",
   "metadata": {},
   "source": [
    "Podemos ver que en esta base de datos tenemos, para cada reseña, los siguientes datos:\n",
    "\n",
    "1. **review_id:** una identificación única para cada reseña.\n",
    "2. **user_id:** un identificador anonimizado para cada usuario que escribió la reseña.\n",
    "3. **business_id:** un identificador anonimizado para cada negocio al que se dirige la reseña.\n",
    "4. **stars:** la calificación en estrellas que cada persona ha estipulado para calificar el negocio.\n",
    "5. **date:** la fecha en la que la reseña se hizo.\n",
    "6. **text:** el texto completo de la reseña.\n",
    "7. **useful:** número de lectores que indicaron que la reseña fue útil.\n",
    "8. **funny:** número de lectores que indicaron que la reseña es divertida o cómica.\n",
    "9. **cool:** número de lectores que indicaron que la reseña es genial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f1db0-a4e8-4693-808e-cf330349768e",
   "metadata": {},
   "source": [
    "## 3. Pre-procesamiento y estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef1c9f-929a-4666-9d93-683800b48c05",
   "metadata": {},
   "source": [
    "Debemos estandarizar los textos o, en caso contrario, al computador le quedará difícil poder entender que algunos textos son parecidos o similares a otros (por ejemplo, entre palabras). Algunos pasos comunes para esto son:\n",
    "\n",
    "1. **Corregir errores simples:** alguna codificación entre textos es diferente o, incluso, los acentos pueden diferir. Por ello, debemos arreglarlo.\n",
    "2. **Creación de características:** en algunos casos nos conviene identificar si las palabras son sujetos o verbos, entre otros.\n",
    "3. **Reemplazar palabras y oraciones completas:** algunas modificaciones de las palabras pueden hacer que se entiendan como diferentes. Por ejemplo, podemos estandarizar \"horriiiiiible\" por \"horrible\", ya que algunas personas pueden escribirlo de la primera manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2d84-02e3-413d-9923-22869e78e360",
   "metadata": {},
   "source": [
    "Para esto, vamos a utilizar la librería `nltk`, la cual tiene las funciones más básicas de NLP. Otra librería útil es `spaCy`, la cual es más moderna y está más centrada en el uso práctico y avanzado en problemas de negocios. Sin embargo, `nltk` es la más útil en términos didácticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573962b3-afa8-4f24-b936-00301f4ebea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e40402-7188-46f5-9e56-ac8a9fb30b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # Natural Language Tool Kit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import plotly\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba842e-a3f9-4b22-a07b-260103f0458d",
   "metadata": {},
   "source": [
    "Vamos a revisar un poco los textos de las reseñas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94093e41-f05a-413b-9747-d636f6e399c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Super simple place but amazing nonetheless. It...\n",
       "1    Small unassuming place that changes their menu...\n",
       "2    Lester's is located in a beautiful neighborhoo...\n",
       "3    Love coming here. Yes the place always needs t...\n",
       "4    Had their chocolate almond croissant and it wa...\n",
       "5    Cycle Pub Las Vegas was a blast! Got a groupon...\n",
       "6    Who would have guess that you would be able to...\n",
       "7    Always drove past this coffee house and wonder...\n",
       "8    Not bad!! Love that there is a gluten-free, ve...\n",
       "9    Love this place!\\n\\nPeggy is great with dogs a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos las primeras diez reseñas\n",
    "AllReviews = data['text']\n",
    "AllReviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512f7344-faad-4be1-8293-80dceb3c989b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Super simple place but amazing nonetheless. It's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard. \\n\\nStaff was very helpful and friendly.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos un \"zoom\" con la primera reseña\n",
    "AllReviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa50bc3-87db-4aa4-9ad0-a838183e8449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Super simple place but amazing nonetheless. It's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard. \\n\\nStaff was very helpful and friendly.\",\n",
       "       \"Small unassuming place that changes their menu every so often. Cool decor and vibe inside their 30 seat restaurant. Call for a reservation. \\n\\nWe had their beef tartar and pork belly to start and a salmon dish and lamb meal for mains. Everything was incredible! I could go on at length about how all the listed ingredients really make their dishes amazing but honestly you just need to go. \\n\\nA bit outside of downtown montreal but take the metro out and it's less than a 10 minute walk from the station.\",\n",
       "       \"Lester's is located in a beautiful neighborhood and has been there since 1951. They are known for smoked meat which most deli's have but their brisket sandwich is what I come to montreal for. They've got about 12 seats outside to go along with the inside. \\n\\nThe smoked meat is up there in quality and taste with Schwartz's and you'll find less tourists at Lester's as well.\",\n",
       "       ...,\n",
       "       'The best vanilla latte I have ever had! The coffee can be a bit different at times and depending on the night your in here it can be a little overwhelming with the live band or open mic. I wish it was more chill... But for coffee shops in the area I think it\\'s great! The people that work there are usually always really nice and helpful. \\n\\nThe food is actually pretty good! I usually stray away from \"vegan this and vegan that\" but I manned up have tried multiple things here and have loved just about everything. My favorite is the cheddar bake sandwich! I gotta be honest when I eat here I feel healthier... Its most likely all in my head but who cares! :)',\n",
       "       \"I've only had their Moca and it was gross! I'm moving around the corner and will update this next time I try something. But other than the horrible Moca the place has a cool coffee shop vibe and great customer service.\",\n",
       "       'Went for dinner last night, food was awesome, especially the onion tart we got for starters.  Drinks were good too, but I got a cranberry lemonade with vodka and it was kind of weak.   For my entree I had the Mediterranean salad and split a mushroom pizza, both of which were very good.\\n\\nThe only negative was the length of time to get our food.  We had a 7 year old boy with us and he was very hungry'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos varias reseñas completas al tiempo\n",
    "AllReviews.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6839b1-bc38-40c2-8c03-becf701d3236",
   "metadata": {},
   "source": [
    "#### 3.1. Tokenización de oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32afd8-e887-4839-964d-8d9e6bac20e1",
   "metadata": {},
   "source": [
    "Como se puede observar, cada una de esas reseñas es _un_ elemento del gran DataFrame de reseñas. Para poder analizar las oraciones y sus relaciones, debemos realizar un proceso llamado **tokenización**. Esto implica convertir cada oración, en este caso, en un elemento separado.\n",
    "\n",
    "Esto se puede lograr por medio del método `nltk.sent_tokenize()` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868e5871-0bec-4ff5-b90b-e584bda1a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super simple place but amazing nonetheless.\n",
      "\n",
      "It's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard.\n",
      "\n",
      "Staff was very helpful and friendly.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Super simple place but amazing nonetheless. It's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard. \\n\\nStaff was very helpful and friendly.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenización de la primera reseña\n",
    "sentences = nltk.sent_tokenize(AllReviews[0])\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()\n",
    "    \n",
    "AllReviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf79d6-a60e-4e2a-821e-f872767fc570",
   "metadata": {},
   "source": [
    "La **tokenización** no es trivial: toma las oraciones hasta el primer punto (donde termina la oración) y la separa.\n",
    "\n",
    "Sin embargo, puede ocurrrir algunos problemas con este método, como, por ejemplo, en la simplificación de palabras. Es decir, si tomáramos el caso de \"Mrs. Londoño\", la tokenización separaría erróneamente la frase. Lo mismo ocurre con el español en el caso de \"Srto. Londoño\". Se debe tener cuidado en estos casos. Más adelante atenderemos este problema con las expresiones regulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a04f0-8007-4df4-956e-20c23caf825e",
   "metadata": {},
   "source": [
    "### 3.2. Tokenización de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7093ca-d15d-4728-a6c2-a8605b2d5c28",
   "metadata": {},
   "source": [
    "También se puede realizar una tokenización por palabras individuales. Para ello, se puede utilizar el método `nltk.word_tokenize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca3fbb5-3763-4f93-afea-5552e353aca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small unassuming place that changes their menu every so often.\n",
      "['Small', 'unassuming', 'place', 'that', 'changes', 'their', 'menu', 'every', 'so', 'often', '.']\n",
      "\n",
      "Cool decor and vibe inside their 30 seat restaurant.\n",
      "['Cool', 'decor', 'and', 'vibe', 'inside', 'their', '30', 'seat', 'restaurant', '.']\n",
      "\n",
      "Call for a reservation.\n",
      "['Call', 'for', 'a', 'reservation', '.']\n",
      "\n",
      "We had their beef tartar and pork belly to start and a salmon dish and lamb meal for mains.\n",
      "['We', 'had', 'their', 'beef', 'tartar', 'and', 'pork', 'belly', 'to', 'start', 'and', 'a', 'salmon', 'dish', 'and', 'lamb', 'meal', 'for', 'mains', '.']\n",
      "\n",
      "Everything was incredible!\n",
      "['Everything', 'was', 'incredible', '!']\n",
      "\n",
      "I could go on at length about how all the listed ingredients really make their dishes amazing but honestly you just need to go.\n",
      "['I', 'could', 'go', 'on', 'at', 'length', 'about', 'how', 'all', 'the', 'listed', 'ingredients', 'really', 'make', 'their', 'dishes', 'amazing', 'but', 'honestly', 'you', 'just', 'need', 'to', 'go', '.']\n",
      "\n",
      "A bit outside of downtown montreal but take the metro out and it's less than a 10 minute walk from the station.\n",
      "['A', 'bit', 'outside', 'of', 'downtown', 'montreal', 'but', 'take', 'the', 'metro', 'out', 'and', 'it', \"'s\", 'less', 'than', 'a', '10', 'minute', 'walk', 'from', 'the', 'station', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se separa por oraciones\n",
    "sentences = nltk.sent_tokenize(data['text'][1])\n",
    "\n",
    "# Se separa por palabras\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90fed4-83b5-4a88-872d-6f623dcf528e",
   "metadata": {},
   "source": [
    "### 3.3. Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc1cdb-d812-4da6-850d-a2f2f9e4679e",
   "metadata": {},
   "source": [
    "Realice un EDA que explore el tamaño de las reseñas: encuentre la más pequeña y la más larga reseña, el promedio y la mediana de palabras y después grafique un histograma mostrando la distribución del tamaño de las reseñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0bb6c02-b3a6-4481-9f7c-6cdd52a6a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica a todas las reseñas la tokenización\n",
    "review_words_lengths = AllReviews.apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb534a9-e17f-4bff-8455-f1b7e14c8c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximo: 1148 \n",
      "Mínimo: 2 \n",
      "Promedio: 154.117 \n",
      "Mediana: 112.0\n"
     ]
    }
   ],
   "source": [
    "# Se muestra cuál es el mínimo y máximo de tamaño de las reseñas\n",
    "print(\"Máximo: {0} \\nMínimo: {1} \\nPromedio: {2} \\nMediana: {3}\".format(max(review_words_lengths), min(review_words_lengths), np.mean(review_words_lengths), np.median(review_words_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b60e1-1a27-48a5-b341-10876318a7dd",
   "metadata": {},
   "source": [
    "Revisemos cuáles son estas oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3968c1-b16b-47bd-8df9-5982cbdbfce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687    在拉斯维加斯买了新房子，本来想安装布艺窗帘， 但是经过小刘的介绍，知道拉斯维加斯特别热，布艺...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La más pequeña reseña\n",
    "AllReviews[review_words_lengths[review_words_lengths == 2].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c78f816-259f-40b3-b49e-bce846709b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright...this is a bit of a mixed bag review-wise...not because there is some good and bad, but because...well...read the review.\n",
      "\n",
      "It was my birthday...or a few days later.  A friend and I were celebrating.  Smallman Galley was on my \"list\" to try.  I didn't know what to expect.  But I think I had in my mind a vision of what this \"chef incubator\" was going to be.  \n",
      "\n",
      "Things I didn't expect:  picnic table seating, 4 \"store fronts\", people lined up ordering from the counter.  So...go into it with that expectation.  I kind of love the idea of the place, but if you were thinking to woo your lady love (or man love, whatevs) with a candle-lit dinner in a cozy restaurant with amazing service...well, you'd  miss the mark.\n",
      "\n",
      "I had to wait.  My friend was late.  I was sitting at the bar, so I didn't really care.  I perused the menu.  First drink that popped out at me was a Primus song.  \"Jerry Was a Race Car Driver\" reportedly had:  Amara Sfumato Rabarbaro, Aperol, Genepy des Alpes, pluot syrup, and La Croix.  \n",
      "\n",
      "First impressions:  I don't know what any of those things are.  Not one.  Hadn't heard of them.  Hadn't had them.  Didn't know how to pronounce them.  The only word I recognized was \"syrup\".  I vaguely recalled 'des' might have meant \"of\" in friench.  Or Belgian.  Whatever.  Completely incomprehensible drink while you wait for a friend?  TAKE ALL MY MONEY!\n",
      "\n",
      "It was really good.  Served in a tall glass with a lemon garnish, I sat and drank my fancy unpronouncably ingrediented drink in suave sophistication.\n",
      "\n",
      "I texted a picture to my friend.  \"You're there already?\"  Yeah...because that's when we said we'd meet.  \n",
      "\n",
      "Whatever.  I finished that drink and went two lines up on the drink menu.  \"Vilified\"  This promised brown butter-washed bourbon, sage-sweet potato syrup, and black walnut bitters.  So much effort went into this drink.  Who do they get to wash the bourbon in brown butter?  How many natives died making the sage-sweet potato syrup?  And I don't even know how you make a bitter let alone a black walnut bitter.  This drink was described so pretentiously I had to lie to the bartender and tell him my name was Thurston J. Howell before he'd pour it for me.  It was great.  I would have had another.  Wait...I did have another.  I think.  Things were starting to get a little...hazy.  It had an orange garnish.  It didn't photograph well. \n",
      "\n",
      "I texted a picture of vilified to my friend.  \"What?  You're on your second drink?\"  I was.  \"You're going to be hammered by the time I get there.\"  I was.\n",
      "\n",
      "Whatever.  I finished that drink and and moved on to \"Cyco Vision\".  Redemption rye, Yellow Chartreuse, pistachio orgeat, lemon, cardamom bitters, egg white.  And listen...I'm a sucker for egg whites in my drink.  I can't explain why.  Rye?  Awesome.  Yellow Chartreuse?  Confusing.  Orgeat?  Let's be honest.  When I got to orgeat it sounded like orgy and I might have chuckled to myself and just ordered the drink because of the orgeat.  I don't even know what orgeat is.  But...it also reminded me of Xergiok from Adventure Time.  I had to have this drink.  \n",
      "\n",
      "I had this drink.  It was good.  It was honestly my least favorite of the three but it was good.  I took a picture...it looked like a malt.  Served in a wide goblet, It had a thick foam on top and the bartender drew a little design with ...maybe with the orgeat...I don't know.  Fancy!  \n",
      "\n",
      "I texted my friend the picture.  \"Jesus, I'm almost there, slow down!\"  I did.  \n",
      "\n",
      "And then my friend arrived and I recommended the Vilified and she hated it, so I drank hers and she took the \"Superman\" that I ordered, which was good, but I wasn't in love with it.  \n",
      "\n",
      "And then the bartender accidentally made someone another drink and it wasn't what he ordered and so she was going to dump it, but I said, \"What do you do with a drink when it's the wrong one?\" and she said, \"Dump it, unless you want it.\"  I did.  I don't even know what it was.  I think it was good. It's not completely clear.\n",
      "\n",
      "And so now...Well now I've had five or six drinks in an hour and a half and I'm pretty much open for business, but it's time to eat, because god forbid we drink on an empty stomach.  TOO LATE!! IT WAS TOO LATE!!  But I walked past the previously-mentioned store fronts on my way to the bathroom and brought back a few ideas...\n",
      "\n",
      "We ended up eating shrimp and grits (fairly certain this was delicious, but I have no idea which place I got it from) and poutine (again...and again).\n",
      "\n",
      "So...I guess the mixed bag is that 1)  I really don't have a clear memory of the food.  I know it was good.  I just don't know where I got it, but 2) I have a very strong sense of the drink menu.  And it was a good drink menu.  \n",
      "\n",
      "Go for drinks!\n",
      "\n",
      "I will be back.  What I remember of the food was that it was really good.  I know what to expect (no waitress, first come first serve seating, picnic table style).  Outstanding place to go with a group of friends and just sort of sprawl out across a picnic table and drink and try new foods.\n"
     ]
    }
   ],
   "source": [
    "# La más larga reseña\n",
    "AllReviews[review_words_lengths[review_words_lengths == 1148].index]\n",
    "print(AllReviews[9349])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b5865f2-2e69-4c89-a787-04fdd3e7ab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograma\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 6\n",
    "review_words_lengths.hist(bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8d2e6-1b01-4c86-9b52-903990574682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a4148c4-b582-4ad3-8b98-8dbd31ef6b31",
   "metadata": {},
   "source": [
    "## 4. Nubes de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a280a-6394-46ed-a9cd-dc2f078ffa4e",
   "metadata": {},
   "source": [
    "La visualización de los datos siempre es importante, como lo hemos visto. Aunque en algunos casos es importante analizar cosas tales como los histogramas y demás gráficas, en el caso de las palabras las nubes también pueden ser útiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aede3a5e-4e55-4363-ae3d-958f77cefe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d409afbd-0071-45c3-b7d6-237cd1828a1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Se importan las librerías necesarias\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Se juntan las palabras de las reseñas\u001b[39;00m\n\u001b[0;32m      6\u001b[0m word_cloud_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Se importan las librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Se juntan las palabras de las reseñas\n",
    "word_cloud_text = ''.join(data.text)\n",
    "\n",
    "# Se construye la nube de palabras\n",
    "wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=800, height=400).generate(word_cloud_text)\n",
    "\n",
    "# Se construye y se muestra la imagen\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f316f2e-a09a-4ba4-968e-6bc255f15019",
   "metadata": {},
   "source": [
    "> **¡Cuidado!** Las nubes de palabras no son siempre la mejor opción, pero ayudan a obtener cierta información de manera rápida y didáctica. Esto puede servir para demostrar rápidamente cuáles son las palabras más repetidas en el texto que estamos analizando. No obstante, en algunas ocasiones puede ser difícil obtener conclusiones estructuradas con este tipo de visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8da4b-3305-43e4-952a-a129a3b12b5f",
   "metadata": {},
   "source": [
    "Ahora hagamos una nube de palabras que muestre una nube de palabras a partir de su calificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e9326-ebcd-4541-ba6e-2e57dafbbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se mejora la resolución para una mejor visualización\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 30, 60\n",
    "\n",
    "def word_cloud_rating(data,star_value):\n",
    "    \n",
    "    data_filtered = data[data.stars == star_value] # Se filtra para la estrella deseada\n",
    "    Reviews = data_filtered.text\n",
    "\n",
    "    Reviews_text = ' '.join(Reviews.values) # Juntando todas las palabras\n",
    "\n",
    "    # Se crea la nube de palabras\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=800, height=400).generate(Reviews_text)\n",
    "\n",
    "\n",
    "    # Se crea y se muestra la imagen\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f4955-ff31-48f6-825d-1f9e6b20bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_rating(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e04ae9-ecf2-4c16-8a4a-ae04dbff70bd",
   "metadata": {},
   "source": [
    "Si analizamos la nube de palabras, la palabra \"good\" (lo que se podría traducir como \"bueno\", en español) aparece repetitivamente en las reseñas con una estrella, lo que parece contraproducente. Echemos un vistazo al respecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa2c13f-9b17-4932-9129-aacbeb7667c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NYC, we had really good ramen. listen\n",
      "nd thought it sound good. First of all, \n",
      " furniture might be good price but not w\n",
      "d Centre and it was good. Sheppard Cen\n",
      "cations. Always had good experiences at \n",
      "e?\" He said it was \"good\". This really s\n",
      "irst time was quite good, but the second\n",
      "l it into something good. I asked that t\n",
      "s place has so many good reviews. That w\n",
      "st perplexed by the good reviews of this\n",
      "g they do have some good regulars. Who, \n",
      "ad that's all we're good for apparently.\n",
      "al, which is always good at any Sambalat\n",
      ". That is where the good part ends. Ther\n",
      "eat right that is a good move until they\n",
      "r food and a pretty good deal for the pr\n",
      "urself does it make good business sense \n",
      "\n",
      "ew was there was no good place to sit an\n",
      "ized which is not a good experience. The\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos aquellos textos que tienen una estrella y la palabra \"good\"\n",
    "reviews_containing_good = [each for each in data[data.stars == 1].text if 'good' in each]\n",
    "\n",
    "for review in reviews_containing_good[:20]:\n",
    "    good_index = review.find(\"good\") # Se encuentra el índice de la palabra\n",
    "    print(review[good_index-20:good_index+20].replace(\"\\n\", \"\")) # Se imprime los 20 caracteres antes y después"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668c5c0-a1e4-4f1d-9cf1-d532b79eb112",
   "metadata": {},
   "source": [
    "### Pequeño ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51650f86-8d42-4f06-b579-973a4dcf06e9",
   "metadata": {},
   "source": [
    "A. ¿Qué vemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fad6a-36bd-466e-92f3-62bed9fc3d68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1e8e9e-4ab0-49f9-87f6-894df22ba842",
   "metadata": {},
   "source": [
    "## 5. N-Grama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76beb38-719c-4ba4-bf1a-68ea83899439",
   "metadata": {},
   "source": [
    "Un **N-grama** es una subsecuencia de _n_ elementos de una secuencia de palabras. Es decir, es una conformación de _n_ palabras tras una palabra en específico, contando la palabra. Por ejemplo, para la frase del gran cantautor contemporáneo Benito Martínez:\n",
    "\n",
    "<center> \"Yo le hablo a Dios y tú eres su respuesta\" </center>\n",
    "\n",
    "Se puede obtener los siguiente n-gramas:\n",
    "\n",
    "* **Unigrama**: \"Yo\", \"Le\", \"Hablo\", entre otros. En este caso `n=1`.\n",
    "* **Bigrama**: \"Yo le\", \"le hablo\", \"hablo a\", \"a Dios\", entre otros. En este caso `n=2`.\n",
    "* **Trigrama**: \"Yo le hablo\", \"le hablo a\", \"hablo a Dios\", entre otros. En este caso `n=3`.\n",
    "\n",
    "Los restantes n-gramas se basan en la cantidad de palabras que ustedes decidan utilizar al cambiar la _n_ como parámetro.\n",
    "\n",
    "Esta forma de organización del lenguaje natural ayuda a manejar la estructura de una frase de mejor manera, permitiendo capturar mejor el contexto e intención de la frase. Esto resulta sorpresivamente útil para entrenar modelos de una manera más eficiente en comparación con el uso de una sola palabra como unidad básica. Sin embargo, la dimensionalidad puede crecer sorpresivamente y peligrosamente.\n",
    "\n",
    "> **Nota:** en este caso, el \"grama\" o \"gramo\" que escogimos son las palabras. También se puede utilizar como unidad básica los caracteres, letras, sílabas, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e125491-7e7d-4c16-9695-38e231c6081b",
   "metadata": {},
   "source": [
    "### 5.1. Conteo de n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77268ce-7a09-4f77-9f60-c3aa4df29103",
   "metadata": {},
   "source": [
    "El conteo de n-gramas ayuda mucho a entender mejor la estructura de un texto. Por ejemplo, podemos ver cuál es la repetición de una palabra (unigrama), dos palabras (bigramas) o tres palabras (trigramas). También se puede analizar desde estructuras aún más grandes y complejas.\n",
    "\n",
    "Normalmente se crea una matriz de co-ocurrencia de palabras, en la que se especifica un \"1\" si la palabra $W$ ocurre en un documento $X$, y \"0\" si no. Creemos una básica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc244abd-2c23-478b-80ea-654002d55d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0014</th>\n",
       "      <th>00429</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00p</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>0146</th>\n",
       "      <th>...</th>\n",
       "      <th>可能說中文啊</th>\n",
       "      <th>在拉斯维加斯买了新房子</th>\n",
       "      <th>布艺隔热性不强</th>\n",
       "      <th>所以用了实木的百叶窗</th>\n",
       "      <th>推荐卓越窗帘给大家</th>\n",
       "      <th>本来想安装布艺窗帘</th>\n",
       "      <th>知道拉斯维加斯特别热</th>\n",
       "      <th>而且价格实惠</th>\n",
       "      <th>隔热</th>\n",
       "      <th>非常漂亮</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31803 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0014  00429  00a  00am  00p  00pm  01  0146  ...  可能說中文啊  \\\n",
       "0   0    0     0      0    0     0    0     0   0     0  ...       0   \n",
       "1   0    0     0      0    0     0    0     0   0     0  ...       0   \n",
       "2   0    0     0      0    0     0    0     0   0     0  ...       0   \n",
       "3   0    0     0      0    0     0    0     0   0     0  ...       0   \n",
       "4   0    0     0      0    0     0    0     0   0     0  ...       0   \n",
       "\n",
       "   在拉斯维加斯买了新房子  布艺隔热性不强  所以用了实木的百叶窗  推荐卓越窗帘给大家  本来想安装布艺窗帘  知道拉斯维加斯特别热  而且价格实惠  \\\n",
       "0            0        0           0          0          0           0       0   \n",
       "1            0        0           0          0          0           0       0   \n",
       "2            0        0           0          0          0           0       0   \n",
       "3            0        0           0          0          0           0       0   \n",
       "4            0        0           0          0          0           0       0   \n",
       "\n",
       "   隔热  非常漂亮  \n",
       "0   0     0  \n",
       "1   0     0  \n",
       "2   0     0  \n",
       "3   0     0  \n",
       "4   0     0  \n",
       "\n",
       "[5 rows x 31803 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TSe importa el módulo a utilizar\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Se crea la matriz\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Se transforma y cuentan todas las palabras en los textos\n",
    "X = vec.fit_transform(AllReviews)\n",
    "\n",
    "# Se crea una matriz en Pandas para visualización\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7d7e8-a360-4e90-b509-43842ae893df",
   "metadata": {},
   "source": [
    "### Pequeño ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda7c2b-38b4-475f-9df3-c673b667e80e",
   "metadata": {},
   "source": [
    "Encontremos las 15 palabras que más se repiten y las 15 que menos se repiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd264e90-1667-4e8e-a4fe-6678e15ffbfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      3\u001b[0m all_reviews_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(AllReviews) \u001b[38;5;66;03m# Junta todo en una cadena de palabras\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenized_words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_reviews_text\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Se tokenizan\u001b[39;00m\n\u001b[0;32m      5\u001b[0m word_freq \u001b[38;5;241m=\u001b[39m Counter(tokenized_words)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mlen\u001b[39m(word_freq)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1381\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;66;03m# Find the word before the current match\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m split \u001b[38;5;241m=\u001b[39m text[: match\u001b[38;5;241m.\u001b[39mstart()]\u001b[38;5;241m.\u001b[39mrsplit(maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1381\u001b[0m before_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(split[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1382\u001b[0m before_words[match] \u001b[38;5;241m=\u001b[39m split[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m matches\u001b[38;5;241m.\u001b[39mappend(match)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_reviews_text = ' '.join(AllReviews) # Junta todo en una cadena de palabras\n",
    "tokenized_words = nltk.word_tokenize(all_reviews_text) # Se tokenizan\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d70ad-56de-42c4-968a-4bcdfead335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.most_common(15) # Se muestran las 15 primeras palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eb64c-6282-4361-b602-f42c5316cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.most_common()[-15:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bd477-dbf4-4018-809a-d947376b9df9",
   "metadata": {},
   "source": [
    "Ahora ustedes encuentren los bigramas que más se repiten (**Nota:** utilicen el método `ngrams` de `nltk.util`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf991b3-d685-49e5-9bad-4ffba232ce69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Super', 'simple'),\n",
       " ('simple', 'place'),\n",
       " ('place', 'but'),\n",
       " ('but', 'amazing'),\n",
       " ('amazing', 'nonetheless'),\n",
       " ('nonetheless', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', \"'s\"),\n",
       " (\"'s\", 'been'),\n",
       " ('been', 'around'),\n",
       " ('around', 'since'),\n",
       " ('since', 'the'),\n",
       " ('the', '30'),\n",
       " ('30', \"'s\"),\n",
       " (\"'s\", 'and'),\n",
       " ('and', 'they'),\n",
       " ('they', 'still'),\n",
       " ('still', 'serve'),\n",
       " ('serve', 'the'),\n",
       " ('the', 'same'),\n",
       " ('same', 'thing'),\n",
       " ('thing', 'they'),\n",
       " ('they', 'started'),\n",
       " ('started', 'with'),\n",
       " ('with', ':'),\n",
       " (':', 'a'),\n",
       " ('a', 'bologna'),\n",
       " ('bologna', 'and'),\n",
       " ('and', 'salami'),\n",
       " ('salami', 'sandwich'),\n",
       " ('sandwich', 'with'),\n",
       " ('with', 'mustard'),\n",
       " ('mustard', '.'),\n",
       " ('.', 'Staff'),\n",
       " ('Staff', 'was'),\n",
       " ('was', 'very'),\n",
       " ('very', 'helpful'),\n",
       " ('helpful', 'and'),\n",
       " ('and', 'friendly.Small'),\n",
       " ('friendly.Small', 'unassuming'),\n",
       " ('unassuming', 'place'),\n",
       " ('place', 'that'),\n",
       " ('that', 'changes'),\n",
       " ('changes', 'their'),\n",
       " ('their', 'menu'),\n",
       " ('menu', 'every'),\n",
       " ('every', 'so'),\n",
       " ('so', 'often'),\n",
       " ('often', '.'),\n",
       " ('.', 'Cool'),\n",
       " ('Cool', 'decor'),\n",
       " ('decor', 'and'),\n",
       " ('and', 'vibe'),\n",
       " ('vibe', 'inside'),\n",
       " ('inside', 'their'),\n",
       " ('their', '30'),\n",
       " ('30', 'seat'),\n",
       " ('seat', 'restaurant'),\n",
       " ('restaurant', '.'),\n",
       " ('.', 'Call'),\n",
       " ('Call', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'reservation'),\n",
       " ('reservation', '.'),\n",
       " ('.', 'We'),\n",
       " ('We', 'had'),\n",
       " ('had', 'their'),\n",
       " ('their', 'beef'),\n",
       " ('beef', 'tartar'),\n",
       " ('tartar', 'and'),\n",
       " ('and', 'pork'),\n",
       " ('pork', 'belly'),\n",
       " ('belly', 'to'),\n",
       " ('to', 'start'),\n",
       " ('start', 'and'),\n",
       " ('and', 'a'),\n",
       " ('a', 'salmon'),\n",
       " ('salmon', 'dish'),\n",
       " ('dish', 'and'),\n",
       " ('and', 'lamb'),\n",
       " ('lamb', 'meal'),\n",
       " ('meal', 'for'),\n",
       " ('for', 'mains'),\n",
       " ('mains', '.'),\n",
       " ('.', 'Everything'),\n",
       " ('Everything', 'was'),\n",
       " ('was', 'incredible'),\n",
       " ('incredible', '!'),\n",
       " ('!', 'I'),\n",
       " ('I', 'could'),\n",
       " ('could', 'go'),\n",
       " ('go', 'on'),\n",
       " ('on', 'at'),\n",
       " ('at', 'length'),\n",
       " ('length', 'about'),\n",
       " ('about', 'how'),\n",
       " ('how', 'all'),\n",
       " ('all', 'the'),\n",
       " ('the', 'listed'),\n",
       " ('listed', 'ingredients'),\n",
       " ('ingredients', 'really'),\n",
       " ('really', 'make'),\n",
       " ('make', 'their'),\n",
       " ('their', 'dishes'),\n",
       " ('dishes', 'amazing'),\n",
       " ('amazing', 'but'),\n",
       " ('but', 'honestly'),\n",
       " ('honestly', 'you'),\n",
       " ('you', 'just'),\n",
       " ('just', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'go'),\n",
       " ('go', '.'),\n",
       " ('.', 'A'),\n",
       " ('A', 'bit'),\n",
       " ('bit', 'outside'),\n",
       " ('outside', 'of'),\n",
       " ('of', 'downtown'),\n",
       " ('downtown', 'montreal'),\n",
       " ('montreal', 'but'),\n",
       " ('but', 'take'),\n",
       " ('take', 'the'),\n",
       " ('the', 'metro'),\n",
       " ('metro', 'out'),\n",
       " ('out', 'and'),\n",
       " ('and', 'it'),\n",
       " ('it', \"'s\"),\n",
       " (\"'s\", 'less'),\n",
       " ('less', 'than'),\n",
       " ('than', 'a'),\n",
       " ('a', '10'),\n",
       " ('10', 'minute'),\n",
       " ('minute', 'walk'),\n",
       " ('walk', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'station.Lester'),\n",
       " ('station.Lester', \"'s\"),\n",
       " (\"'s\", 'is'),\n",
       " ('is', 'located'),\n",
       " ('located', 'in'),\n",
       " ('in', 'a'),\n",
       " ('a', 'beautiful'),\n",
       " ('beautiful', 'neighborhood'),\n",
       " ('neighborhood', 'and'),\n",
       " ('and', 'has'),\n",
       " ('has', 'been'),\n",
       " ('been', 'there'),\n",
       " ('there', 'since'),\n",
       " ('since', '1951'),\n",
       " ('1951', '.'),\n",
       " ('.', 'They'),\n",
       " ('They', 'are'),\n",
       " ('are', 'known'),\n",
       " ('known', 'for'),\n",
       " ('for', 'smoked'),\n",
       " ('smoked', 'meat'),\n",
       " ('meat', 'which'),\n",
       " ('which', 'most'),\n",
       " ('most', 'deli'),\n",
       " ('deli', \"'s\"),\n",
       " (\"'s\", 'have'),\n",
       " ('have', 'but'),\n",
       " ('but', 'their'),\n",
       " ('their', 'brisket'),\n",
       " ('brisket', 'sandwich'),\n",
       " ('sandwich', 'is'),\n",
       " ('is', 'what'),\n",
       " ('what', 'I'),\n",
       " ('I', 'come'),\n",
       " ('come', 'to'),\n",
       " ('to', 'montreal'),\n",
       " ('montreal', 'for'),\n",
       " ('for', '.'),\n",
       " ('.', 'They'),\n",
       " ('They', \"'ve\"),\n",
       " (\"'ve\", 'got'),\n",
       " ('got', 'about'),\n",
       " ('about', '12'),\n",
       " ('12', 'seats'),\n",
       " ('seats', 'outside'),\n",
       " ('outside', 'to'),\n",
       " ('to', 'go'),\n",
       " ('go', 'along'),\n",
       " ('along', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'inside'),\n",
       " ('inside', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'smoked'),\n",
       " ('smoked', 'meat'),\n",
       " ('meat', 'is'),\n",
       " ('is', 'up'),\n",
       " ('up', 'there'),\n",
       " ('there', 'in'),\n",
       " ('in', 'quality'),\n",
       " ('quality', 'and'),\n",
       " ('and', 'taste'),\n",
       " ('taste', 'with'),\n",
       " ('with', 'Schwartz'),\n",
       " ('Schwartz', \"'s\"),\n",
       " (\"'s\", 'and'),\n",
       " ('and', 'you'),\n",
       " ('you', \"'ll\"),\n",
       " (\"'ll\", 'find'),\n",
       " ('find', 'less'),\n",
       " ('less', 'tourists'),\n",
       " ('tourists', 'at'),\n",
       " ('at', 'Lester'),\n",
       " ('Lester', \"'s\"),\n",
       " (\"'s\", 'as'),\n",
       " ('as', 'well.Love'),\n",
       " ('well.Love', 'coming'),\n",
       " ('coming', 'here'),\n",
       " ('here', '.'),\n",
       " ('.', 'Yes'),\n",
       " ('Yes', 'the'),\n",
       " ('the', 'place'),\n",
       " ('place', 'always'),\n",
       " ('always', 'needs'),\n",
       " ('needs', 'the'),\n",
       " ('the', 'floor'),\n",
       " ('floor', 'swept'),\n",
       " ('swept', 'but'),\n",
       " ('but', 'when'),\n",
       " ('when', 'you'),\n",
       " ('you', 'give'),\n",
       " ('give', 'out'),\n",
       " ('out', 'peanuts'),\n",
       " ('peanuts', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'shell'),\n",
       " ('shell', 'how'),\n",
       " ('how', 'wo'),\n",
       " ('wo', \"n't\"),\n",
       " (\"n't\", 'it'),\n",
       " ('it', 'always'),\n",
       " ('always', 'be'),\n",
       " ('be', 'a'),\n",
       " ('a', 'bit'),\n",
       " ('bit', 'dirty'),\n",
       " ('dirty', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'food'),\n",
       " ('food', 'speaks'),\n",
       " ('speaks', 'for'),\n",
       " ('for', 'itself'),\n",
       " ('itself', ','),\n",
       " (',', 'so'),\n",
       " ('so', 'good'),\n",
       " ('good', '.'),\n",
       " ('.', 'Burgers'),\n",
       " ('Burgers', 'are'),\n",
       " ('are', 'made'),\n",
       " ('made', 'to'),\n",
       " ('to', 'order'),\n",
       " ('order', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'meat'),\n",
       " ('meat', 'is'),\n",
       " ('is', 'put'),\n",
       " ('put', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'grill'),\n",
       " ('grill', 'when'),\n",
       " ('when', 'you'),\n",
       " ('you', 'order'),\n",
       " ('order', 'your'),\n",
       " ('your', 'sandwich'),\n",
       " ('sandwich', '.'),\n",
       " ('.', 'Getting'),\n",
       " ('Getting', 'the'),\n",
       " ('the', 'small'),\n",
       " ('small', 'burger'),\n",
       " ('burger', 'just'),\n",
       " ('just', 'means'),\n",
       " ('means', '1'),\n",
       " ('1', 'patty'),\n",
       " ('patty', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'regular'),\n",
       " ('regular', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', '2'),\n",
       " ('2', 'patty'),\n",
       " ('patty', 'burger'),\n",
       " ('burger', 'which'),\n",
       " ('which', 'is'),\n",
       " ('is', 'twice'),\n",
       " ('twice', 'the'),\n",
       " ('the', 'deliciousness'),\n",
       " ('deliciousness', '.'),\n",
       " ('.', 'Getting'),\n",
       " ('Getting', 'the'),\n",
       " ('the', 'Cajun'),\n",
       " ('Cajun', 'fries'),\n",
       " ('fries', 'adds'),\n",
       " ('adds', 'a'),\n",
       " ('a', 'bit'),\n",
       " ('bit', 'of'),\n",
       " ('of', 'spice'),\n",
       " ('spice', 'to'),\n",
       " ('to', 'them'),\n",
       " ('them', 'and'),\n",
       " ('and', 'whatever'),\n",
       " ('whatever', 'size'),\n",
       " ('size', 'you'),\n",
       " ('you', 'order'),\n",
       " ('order', 'they'),\n",
       " ('they', 'always'),\n",
       " ('always', 'throw'),\n",
       " ('throw', 'more'),\n",
       " ('more', 'fries'),\n",
       " ('fries', '('),\n",
       " ('(', 'a'),\n",
       " ('a', 'lot'),\n",
       " ('lot', 'more'),\n",
       " ('more', 'fries'),\n",
       " ('fries', ')'),\n",
       " (')', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'bag.Had'),\n",
       " ('bag.Had', 'their'),\n",
       " ('their', 'chocolate'),\n",
       " ('chocolate', 'almond'),\n",
       " ('almond', 'croissant'),\n",
       " ('croissant', 'and'),\n",
       " ('and', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'amazing'),\n",
       " ('amazing', '!'),\n",
       " ('!', 'So'),\n",
       " ('So', 'light'),\n",
       " ('light', 'and'),\n",
       " ('and', 'buttery'),\n",
       " ('buttery', 'and'),\n",
       " ('and', 'oh'),\n",
       " ('oh', 'my'),\n",
       " ('my', 'how'),\n",
       " ('how', 'chocolaty'),\n",
       " ('chocolaty', '.'),\n",
       " ('.', 'If'),\n",
       " ('If', 'you'),\n",
       " ('you', \"'re\"),\n",
       " (\"'re\", 'looking'),\n",
       " ('looking', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'light'),\n",
       " ('light', 'breakfast'),\n",
       " ('breakfast', 'then'),\n",
       " ('then', 'head'),\n",
       " ('head', 'out'),\n",
       " ('out', 'here'),\n",
       " ('here', '.'),\n",
       " ('.', 'Perfect'),\n",
       " ('Perfect', 'spot'),\n",
       " ('spot', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'coffee\\\\/latté'),\n",
       " ('coffee\\\\/latté', 'before'),\n",
       " ('before', 'heading'),\n",
       " ('heading', 'out'),\n",
       " ('out', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'old'),\n",
       " ('old', 'port')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "first_5_revs = data.text[0:5]\n",
    "word_tokens = nltk.word_tokenize(''.join(first_5_revs))\n",
    "list(ngrams(word_tokens, 2)) #ngrams(word_tokens,n) gives the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f6c76-bc13-47f7-92ba-c8d5bb4359bd",
   "metadata": {},
   "source": [
    "### 6. Stopwords (Palabras vacías)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f1bbd-47f6-4fcd-9400-a6634679d288",
   "metadata": {},
   "source": [
    "Como hemos visto, hay un grupo de palabras que se repiten constantemente, pero que no aportan mucho acerca del fondo del mensaje que se quiere emitir. Estas son palabras necesarias para la construcción del lenguaje natural entre humanos, pero puede confundir el análisis de las máquinas. Estas palabras son pronombres como \"yo\", \"él\", \"ella\"; preposiciones como \"y\", \"pero\"; artículos como \"la\", \"le\", \"lo\", entre otros. Estas palabras son relativamente vacías, por lo que se le llaman \"palabras vacías\" en español y \"stopwords\" en inglés.\n",
    "\n",
    "Por esta razón es común preprocesar los textos para remover este tipo de palabras que no agregan mucho al análisis. Inclusive removerlas puede permitirnos revisan n-gramas de manera más eficiente y efectiva.\n",
    "\n",
    "La librería `nltk` tiene, precisamente, una lista de palabras vacías (_stopwords_) predeterminadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df5799ee-f548-4ddd-98b5-fdd30249ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ef58c5e-4502-4ec9-ae54-ac3698989f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "# Palabras vacías\n",
    "print(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156b1aa-c134-4368-8618-1ead021cd48f",
   "metadata": {},
   "source": [
    "Utilicemos esto para remover aquello que no es importante para nosotros en la primera reseña:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96f71bae-dc4b-4c08-be7e-ed4fb8d31780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'been', 'the', 'and', 'they', 'the', 'same', 'they', 'with', 'a', 'and', 'with', 'was', 'very', 'and']\n",
      "\n",
      "['Super', 'simple', 'place', 'amazing', 'nonetheless', '.', 'It', \"'s\", 'around', 'since', '30', \"'s\", 'still', 'serve', 'thing', 'started', ':', 'bologna', 'salami', 'sandwich', 'mustard', '.', 'Staff', 'helpful', 'friendly', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "without_stop_words = []\n",
    "stopword = []\n",
    "sentence = data.text[0]\n",
    "words = nltk.word_tokenize(sentence)\n",
    "for word in words:\n",
    "    if word in stop_words:\n",
    "        stopword.append(word)\n",
    "    else:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(stopword)\n",
    "print()\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d29592-08e2-446b-b475-3dfeef2a4ba9",
   "metadata": {},
   "source": [
    "### Pequeño ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836d9f0-8049-4b1a-b4d7-c958ba05abe7",
   "metadata": {},
   "source": [
    "Utilizando la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67eef49a-4195-4bc4-9ba6-ffdb3969ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "    \n",
    "   # x_pos = [k for k,v in most_common_k]\n",
    "   # y_pos = [v for k,v in most_common_k]\n",
    "    \n",
    "   # plt.bar(x_pos, y_pos,align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fd5a3-c00a-42bd-9c1f-1cba40bca538",
   "metadata": {},
   "source": [
    "Encuentre las palabras más repetidas sin las palabras vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d823b2-1f0d-4d41-88fe-0759b53a125d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_1000_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m eng_stopwords \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m### Getting a single string\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m all_reviews_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mtop_1000_reviews\u001b[49m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m## Splitting them into tokens\u001b[39;00m\n\u001b[0;32m     13\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(all_reviews_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_1000_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Removing the most basic stop words from the ntlk corpus and including only those\n",
    "# words with character size above 2 so as to remove punctuations\n",
    "# But, this could be extended to remove further high and low frequency stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "### Getting a single string\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Removing the stopwords\n",
    "word_tokens_clean = [each for each in word_tokens if each.lower() not in eng_stopwords and len(each.lower()) > 2]\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens_clean, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a325a-25de-4e4f-8583-07c3ff3741ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
